{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMIV5UkfDgJX"
      },
      "outputs": [],
      "source": [
        "#Install libraries\n",
        "!pip install datasets\n",
        "!pip install transformers\n",
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Wandb - Create and login to https://wandb.ai/ and paste the access token\n",
        "import wandb\n",
        "wandb.login()\n"
      ],
      "metadata": {
        "id": "DPizFHvcjDbj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, TrainingArguments, Trainer\n",
        "import torch\n",
        "from torch.utils.data import random_split"
      ],
      "metadata": {
        "id": "GxC-j6ezD-Pc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load dataset from hugging face - https://huggingface.co/datasets/demelin/moral_stories\n",
        "dataset=load_dataset('demelin/moral_stories','full')"
      ],
      "metadata": {
        "id": "YRWZPf0eriBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = dataset['train']['norm'][:10000]\n",
        "X_test = dataset['train']['norm'][:-2000]\n",
        "print(\"Total Dataset - \", len(dataset['train']))\n",
        "print(\"Train Dataset - \",len(X_train),\"Test Dataset - \",len(X_test))"
      ],
      "metadata": {
        "id": "DJOlmDx6FgdF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define tokenizer and model\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2', bos_token='<|startoftext|>',\n",
        "                                          eos_token='<|endoftext|>', pad_token='<|pad|>')\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2').cuda()\n",
        "model.resize_token_embeddings(len(tokenizer))"
      ],
      "metadata": {
        "id": "f9mNrpLmsE0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = max([len(tokenizer.encode(x)) for x in X_train])"
      ],
      "metadata": {
        "id": "IuN-5uLasSMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class moral():\n",
        "    def __init__(self, x, tokenizer, max_length):\n",
        "        self.input_ids = []\n",
        "        self.attn_masks = []\n",
        "        self.labels = []\n",
        "        for txt in x:\n",
        "            encodings_dict = tokenizer('<|startoftext|>' + txt + '<|endoftext|>', truncation=True,\n",
        "                                       max_length=max_length, padding=\"max_length\")\n",
        "            self.input_ids.append(torch.tensor(encodings_dict['input_ids']))\n",
        "            self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.attn_masks[idx]\n"
      ],
      "metadata": {
        "id": "Iuxqg5vysepO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = moral(X_train, tokenizer, max_length=max_length)\n",
        "train_size = int(0.8 * len(dataset))\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, len(dataset) - train_size])"
      ],
      "metadata": {
        "id": "ezewCZG9tL2c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define Training parameters\n",
        "training_args = TrainingArguments(output_dir='./results', num_train_epochs=1, logging_steps=100, save_steps=5000,\n",
        "                                  per_device_train_batch_size=1, per_device_eval_batch_size=1,learning_rate = 50**-5,\n",
        "                                  warmup_steps=10, weight_decay=0.05, logging_dir='./logs',report_to=\"wandb\",  run_name=\"gpt-2-test\" )"
      ],
      "metadata": {
        "id": "W_WnEBFSuAQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %env WANDB_WATCH=all\n",
        "# %env WANDB_SILENT=true"
      ],
      "metadata": {
        "id": "10i4dsIqs0k4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train the model and tracking the performance in wandb. After training the model is saved in results folder\n",
        "Trainer(model=model, args=training_args, train_dataset=train_dataset, \n",
        "        eval_dataset=val_dataset, data_collator=lambda data: {'input_ids': torch.stack([f[0] for f in data]),\n",
        "                                                              'attention_mask': torch.stack([f[1] for f in data]),\n",
        "                                                              'labels': torch.stack([f[0] for f in data])}).train()\n",
        "wandb.finish()\n"
      ],
      "metadata": {
        "id": "yzGEoXwjuKYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Inference using sample data\n",
        "tokens = tokenizer(\"It's responsible to\", return_tensors=\"pt\").input_ids.cuda()"
      ],
      "metadata": {
        "id": "jjQIrNujF4z_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Predicted logits with inference parameters\n",
        "predToken = model.generate(tokens, do_sample=True, top_k=50, \n",
        "                          max_length=300, top_p=0.95, temperature=1.9, num_return_sequences=5)"
      ],
      "metadata": {
        "id": "Xc9eF8xMGPf7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Decoding predicted sentence\n",
        "for i, predToken in enumerate(predToken):\n",
        "    print(\"{}: {}\".format(i, tokenizer.decode(predToken, skip_special_tokens=True)))"
      ],
      "metadata": {
        "id": "ZVjmCpsgKEJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluation - Function to generate multiple sentences. Test data should be a dataframe\n",
        "def text_generation(test_data):\n",
        "  generated = []\n",
        "  for sentence in tqdm(test_data):\n",
        "    sentenceList = sentence.split(\" \")\n",
        "    partialSentence = sentenceList[:(len(sentenceList)//2)]\n",
        "    partialSentence = ' '.join(partialSentence)\n",
        "    tokens = tokenizer(partialSentence, return_tensors=\"pt\").input_ids.cuda()\n",
        "    output = model.generate(tokens, do_sample=True, top_k=50,max_length=300, top_p=0.95, temperature=1.9,num_return_sequences=1)\n",
        "    genText = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    generated.append(genText)\n",
        "  return generated\n",
        "\n",
        "#Run the functions to generate the lyrics\n",
        "TestGen = text_generation(X_test[:20])"
      ],
      "metadata": {
        "id": "HBeI5hjD5Igr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Using BLEU score to compare the real sentences with the generated ones\n",
        "import statistics\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "scores=[]\n",
        "\n",
        "for generated,test in tqdm(zip(TestGen,X_test)):\n",
        "  splitGenerated=generated.split(\" \")\n",
        "  reference = [item for item in splitGenerated if item not in splitGenerated[:len(splitGenerated)//2]]\n",
        "  reference = ' '.join(reference)\n",
        "  splitTest = test.split(\" \")\n",
        "  candidate = [item for item in test if item not in test[:len(test)//2]]\n",
        "  candidate = ' '.join(candidate)\n",
        "  bleu = sentence_bleu([reference], candidate, weights = [1])\n",
        "  scores.append(bleu)\n",
        "\n",
        "print('Bleu score - ',statistics.mean(scores))"
      ],
      "metadata": {
        "id": "m3pZgu-yLiA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Pk-H4wLlxftG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}